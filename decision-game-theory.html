
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>new life</title>
            <link rel="stylesheet" href="site.css">
        </head>
        <body>
            <div id="wrapper">
                <h1>Formal models of decision-making</h1>
<p><em>"Wouldn't it be cool to have systematic, predictable, accurate decision-making under uncertainty?"</em> said every systematizer ever. We have good and bad news. The good news is that lots of people want this, and they started making theories about it. The bad news is that this is really hard.</p>
<hr />
<p>The <strong>Nash Equilibrium</strong> is the strategy for a game such that, holding the other players' strategies constant, there would be no incentive for the player to change strategy. <em>Hmm I actually don't properly get the details here</em></p>
<ul>
<li>
<p>For example, you and 20 people are playing a game where you pick a number between 0 and 100, such that your guess will be the closest guess to 2/3 times the mean of everyone's guesses. Normally this is kind of difficult -- you have to simulate everyone else in your head, then simulate them simulating you, then simulate them simulating you simulating them, et cetera, then guess what level the others are simulating you at, and then use that to pick a number $n$ for $(2/3)^n * 50$ -- but there's a strategy where everyone can win instead: <em>just pick 0</em>. </p>
</li>
<li>
<p>If everyone picks 0 in the first round of this game, and then you decide to play a second one, no one has a reason to unilaterally change their guess -- if they do, they'll be ~guaranteed not to win. Hence, once everyone picks 0, you're at a stable strategic state: the Nash Equilibrium. </p>
</li>
</ul>
<p>You could argue that the Nash Equilibrium is generally the "rational choice" for an agent, since it's the only place where you don't have an incentive to change strategy, but for the classical single-round prisoner's dilemma, the Nash Equilibrium is to defect. If either player cooperates the other has an incentive to change strategy and defect.</p>
<p>Can't we do better than this?</p>
<p>Game theory is what we call modeling games like these as a whole: you're modeling multiple agents, looking at their interactions. Decision theories, on the other hand, are about modeling different formal algorithms for <em>individual</em> decisions.</p>
<p>This is a bunch of notes on this and related topics.</p>
<h2>Decision theories</h2>
<p><em>This section is in progress.</em></p>
<p>If you want to formalize reasoning and decision-making, make your own decision theory. It's fun! Try it at home!</p>
<p>Sources: 
- <a href="https://arbital.com/p/logical_dt/?l=5gc">Introduction to Logical Decision Theory</a></p>
<p>The typical conventional reasoning we might use about the world is what we'd describe as <strong>Causal Decision Theory</strong>, or CDT. CDT evaluates decisions for an agent that is distinct from its environment at a specific point in time (i.e. the theory doesn't consider itself "part of the world," which is important if you start doing weird things like blackmail and acausal trades), making decisions based on its predictions about how well things will turn out. </p>
<p>In other words, you hold everything else constant, and look at the agen'ts decision in isolation; you look at the world in which the agent makes decision A as opposed to decision B, you compare the outcomes, and decide based on which one looks better -- which one has higher <em>expected utility</em>.</p>
<p>It's really not that deep. Literally, "look at the causal consequences of your actions and decide which consequences you like better."</p>
<p>But there are outcomes where this formal theory doesn't match our intuitions.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> The typical example is that an economist can argue that, based on CDT, that it's pointless for any given individual to vote; it's impossibly unlikely that a democratic election with tens of thousands of voters will be swung by <em>your vote</em> in particular, nevermind one with millions of votes. It's so unlikely that it's not worth it to even vote in the first place -- the wasted time is a net negative in expectation.</p>
<p>You'd use an expected utility function: $$\mathbb{E}(a_{x}) = \sum_{o \in \mathcal{O}} \mathbb{U}(o_{i}) * P(o_{i}|a_{x})$$</p>
<p>Where</p>
<ul>
<li>$\mathbb{E}(a_{x})$ is the expected utility of an action ${a_x}$</li>
<li>$\mathbb{U}(o_{i})$ is the utility of the outcome ${o_i}$</li>
<li>$P(o_{i}|a_{x})$ is the probability of the outcome $o_i$ if the action $a_x$ is taken.</li>
</ul>
<p>I don't really like this. I like voting in elections. I'd like a formalization of rationality that says you should vote in elections, since my imagination of a "perfectly rational society" filled with rational agents where everyone follows perfect systematic rationality (which is what we're trying to create here, presumably) is <em>not</em> one in which no one votes in elections.</p>
<p>The rest of decision theory is weirder and different in order to try and account for this. We can group all these decision theories under the name <strong>Logical decision theories</strong>.</p>
<p>. . .</p>
<p>If we want to do better than Defect-Defect in the classic prisoner's dilemma -- where you're in two different rooms, you'll need <em>acausal trades</em>. Maybe. Maybe there are other ways to do this or something that aren't absurd, but this one's kind of interesting and fun.</p>
<h2>Game theory with computers</h2>
<p><em>This section is in progress.</em></p>
<p>from <a href="http://arxiv.org/abs/1602.04184">Parametric Bounded Lob's Theorem and Robust Cooperation of Bounded Agents</a></p>
<ul>
<li>We can have a version of the prisoner's dilemma, but instead of two people playing it, it's two computers playing it, and they each get access to each other's source code -- i.e. they can perfectly simulate one another. (I think, maybe there's some weird edge case here)</li>
<li>Fun note: if you have a formally-defined program, it can be rewritten in math or something. Since the computer program is deterministic, we can use proofs to determine the outcome of our bot. Specifically: <pre><code>def FairBot_k(Opponent):
    search for a proof of length k characters that 
    Opponent(FairBot_k) = C

    if found,  
        return C  
    else  
        return D
</code></pre>
</li>
</ul>
<p>(We bound it, I'm assuming, so that the program is guaranteed to terminate.)</p>
<p>This is great. What if we put FairBot against itself? It looks for a proof of length k that the opponent will cooperate, and in order to figure it out must find a proof that its opponent will find a proof that itself finds a proof that its opponent...</p>
<p>Seemingly this goes infinitely, and since we have bounded things at length <code>k</code> the program will terminate without finding a proof and fail. </p>
<p>Actually this is not the case. There's some interesting math around provability that shows us why, called <strong>Lob's theorem</strong>. Using Lob's theorem, we can create "robust cooperative program equilibria for computationally bounded agents" -- this means unexploitable agents in this computer-based prisoner's dilemma game.</p>
<p>The statement "$\Box_{k} p$" means, "$p$ has a proof of $k$ or fewer characters."</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>As is always the case.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
            </div>
            <script type="text/javascript" src="mathjax.js"></script>
            <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
        </body>
        </html>
        