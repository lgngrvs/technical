
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>new life</title>
            <link rel="stylesheet" href="site.css">
            <link rel="stylesheet" href="theme.css">
        </head>
        <body>
            <div id="wrapper">
                <h1>Information theory</h1>
<h3>What actually is information lol</h3>
<p>It's been described to me as, "information resolves uncertainty." I think the best way to get a sense for it is talking about surprisal with regards to a specific thing.</p>
<p>You come to a fork in the road and you have no ideas about which way might be the correct way to go. We can represent them as paths $A$ and $B$. A natural way to represent this is with binary labels, hence it would be instead path $0$ for $A$ and path $1$ for $B$. Intuitively, if someone perfectly trustworthy tells you which way to go, they would be be giving you 1 bit (binary digit) of information: they're giving you a 0 or 1 to distinguish the options in the binary choice. </p>
<p>What if you do have some information preceding this, though? What if you have 70% confidence that path $A$ is correct, since someone earlier on the road who seemed somewhat trustworthy told you it was? They wouldn't be giving you still 1 bit of information, because you'd be updating less — you're less surprised. You already thought this was the case, they're just upgrading your confidence. No longer can we work with simple integers of bits; we realize from this that the amount of information you get is directly related to how much knowledge you already have — or don't have.</p>
<p>The intuition here is that <strong>the <em>amount</em> of information you're getting is related to <em>how surprised you are</em> to receive it.</strong> </p>
<p>In a formal sense, surprisal quantifies <em>how wrong your predictions were.</em> (At least, this is one way to think about it — a starter intuition, if you will.) When an event you didn't expect occurs — i.e. it surprises you — that surprise tells you that your model of the world was wrong or incomplete, insofar as you didn't predict the event with 99.99% confidence. Hence, you most likely have to update your internal model of the world such that it would predict that outcome with higher confidence. Surprise tells you that you're receiving information about the world, about how your model was wrong.</p>
<p>If you have a perfect internal world model — i.e. you're literally omniscient and you can simulate every single possible causal relation in the universe at once — your model will never update, and you'll never be surprised; you know all the information! Conversely, if your model of the world is consistently horrible, you'll be constantly updating your model after your predictions are falsified, constantly surprised at the world. You know very little of the information.</p>
<p>I'll talk about how to measure error and surprise soon, but for now I want to finish formalizing information — I'll derive intuitively the way you actually calculate the information you receive from an event's occurrence.</p>
<h3>Two incorrect intuitions</h3>
<p>I'll start with two ways that you might naturally think about information as a mathematical object, and why those ways lead you to computational problems.</p>
<p>From the 70%/30% situation above, you might think that the person on the path would be giving you 0.3 bits of information, thus making up for the difference in the outcomes. (This would be incorrectly applying intuitions that "probabilities should sum to 1.") If you think about this more deeply, though, the math doesn't work out; distinguishing between two perfectly uncertain options — events that have priors of 0.5 each — would only give you 0.5 bits of information, as opposed to the 1 bit we would expect. Shouldn't someone revealing to you a bit — a zero or a one, a binary choice — give you one bit of information, if you have literally no idea what that bit might be?</p>
<p>Here's another way that "adding to 1" doesn't work out nicely: if you have two fair coins and you flip them both separately, each coin flip would theoretically give you 0.5 bits of information. However, if you looked at it from the perspective of their joint probability distribution — for example, $P(\text{2 heads}) = 0.5 * 0.5 =  0.25$ — we would get 0.75 bits of information instead. This just straightforwardly doesn't work out. Two independent events should not give different amounts information just based on whether you look at their probabilities together or separately — but we just saw that happen! We have to keep looking.</p>
<p>Okay, so information for an event $x$ can't just be $1-P(x)$. What else could it be?</p>
<p>If we think about it, the information we get is quantifying <em>how wrong we are.</em> If event $x$ happens and we predicted it with very high confidence, the amount of information should be really low — we expected it, and aren't very surprised. But conversely, if we predicted $x$ with really low confidence, the amount of information should be very large — and it should scale with orders of magnitude. If our $P(x) = 10\%$ and $x$ occurs, this should be very very different from if our $P(x) = 0.1\%$ — orders of magnitude different. So maybe we could say information is proportional to $1/P(x)$? </p>
<p>That's <em>almost</em> right, but we're still missing something. </p>
<p>To see what's wrong let's imagine two fair dice. The probability of rolling a one on die $A$ is $1/6$; for $B$ it's also $1/6$. If we use $1/P(x)$, and look at the dice independently, seeing snake eyes — two ones — (or any other outcome for that matter) would give us $6 + 6 = 12$ bits. But if we once again consider the joint probability distribution, and ask what the probability of rolling snake eyes is, that probability is $1/6 \cdot 1/6 = 1/32$ — so if we once again rolled snake eyes, we'd get 32 bits of information instead of 12! </p>
<p>What's going on here? We once again just combined two independent events together into one observation without changing their independence, without changing the probability — and the amount information changed. The problem is that we want information to change additively, not multiplicatively; when you "get more information" from an event your knowledge doesn't double, but increases linearly — at the same times as <em>probability</em>, the quantity we want to base our observations on, scales multiplicatively.</p>
<p>To go back to the dice scenario: when we look at the joint distribution of the two dice, or three dice, or $n$ dice, the probability of one specific combination of those dice will be $1/6^n$. But we want the information to increase linearly as the probability decreases multiplicatively. If we wanted to linearize our probability graph, what would we do?</p>
<p>Take the $\log$ of it!</p>
<p>Since we're working in binary already (with bits of information), we can use the $\log_2$ scale — our probability will be $P(x)$ and our information will be $\log_2(1/P(x))$. Ta-da! Here's our information equation: $$I[x] = \log_2(\frac{1}{P(x)}) = -\log_2(P(x))$$
(Sometimes it's nicer to use a negative sign instead of an inverse, but they're equivalent.)</p>
<p>To sum it up, we use $1/P(x)$ because we want information to be inversely proportional to probability — the occurrence of a (in our minds) low-probability event should give us lots of information, and the reverse for high-probability events. Then we add the $\log$ so that information will <em>add</em> when we look at multiple events together, instead of multiplying — it'll accumulate nicely over time instead of exponentiating rapidly.</p>
<p>This equation gives us nice properties: </p>
<ul>
<li>We get 1 bit of information from perfect uncertainty between 2 outcomes: $\log_2(1/2) = 1$</li>
<li>When we are looking at the coins, the information is the same whether or not we look at the joint probability distribution. $-\log_2(0.25) = 2 (-\log(0.5))= 2$.</li>
</ul>
<p><img alt="Information Graph" src="information-graph.png" />
<em><strong>Fig 1:</strong> The graph of $y = \log_2(x)$, where $y$ is the "information content" or the surprisal produced by the occurrence of an event whose probability in your mind is $x$.</em></p>
<p>So there's the idea about why the information content of an event uses a log scale. To sum it up, $\text{Information} = -\log_2P(x)$ where $x$ is an outcome and $P(x)$ is the probability of that outcome. (Sorry to mix notations with $x$ being probability and $x$ being an outcome — hopefully that's not too annoying.)</p>
<h3>Entropy</h3>
<p>We might want to look at information on the level of a probability distribution. We've talked about the information content of specific events, but how do we talk about our beliefs about an event — our internal distributions?</p>
<p>One thing we can talk about is <em>entropy.</em> Entropy in information theory describes something like, "How predictable is this distribution? How <em>surprised</em> will I be on average by an outcome?" In general, entropy measures the predictability of a distribution. (This is not a notion limited to information theory. For example, in statistical mechanics, the entropy of a particular macrostate/thermodynamic state is the number of possible microstates that could produce that macrostate — in other words, the unpredictability of the precise arrangements of atoms for a given macrostate; the unpredictability of the particular positions of atoms, summed over the distribution of all of them.)</p>
<p>Intuitively, to talk about <strong>how much we expect to be surprised</strong> given a distribution of the probabilities of possible outcomes of an event, we can do a simple expected value calculation: take the sum of the information content of each possible outcome, weighted by the probability of the outcome.</p>
<p>Hence, we can describe "expected surprise", AKA Shannon entropy ($H$<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>, for a probability distribution $X$ using the following formula:
$$ H(X) =\sum_{\text{all outcomes in x}} \text{P(outcome)} \cdot  \text{Info if outcome occurs}$$</p>
<p>Written in variables:
$$H(X) =\sum_{x \in X} P(x) \space \log_2 \frac{1}{P(x)}$$
or $$ H(X) =-\sum_{x \in X} P(x) \space \log_2P(x) $$
where $X$ is a distribution of individual events $x$ and $P(x)$ is the probability you assign to the event. Hence, this sum represents going across each possible event in the distribution and computing the value $P(x) * \log_2(1/P(x)$ — corresponding to the info given by an outcome, weighted by how likely it is — then summing that up for each distribution. </p>
<p><img alt="Entropy Distribution" src="entropy-distribution.png" />
<em><strong>Fig 2:</strong> The graph of $-x * \log_2(x)$, which you can call the "expected surprisal" — the probability of the outcome occurring times the surprise that you would receive if it did occur. Notice how the graph is skewed right, but low-probability events, even though they have high surprisal, will have low expected surprisal because they're so unlikely.</em></p>
<p>Higher entropy to me intuitively means <em>less</em> predictability. For example, for the distribution $$X: \{x_1 = 0.9, x_2 = 0.05, x_3 = 0.05\}$$ the entropy is $$-[(0.9 *  \log_2(0.9)) + 2 * (0.05 * \log_2(0.05)] \approx 0.473$$ — whereas for the less-predictable distribution $$X: \{x_1 = 0.33, x_2 = 0.33, x_3 = 0.34\}$$ the entropy is $$-(2 * (0.33 * \log_2(0.33)) + (0.34 * \log_2(0.34)) \approx 1.585.$$</p>
<h2>Comparing distributions</h2>
<p>Okay, we know how to measure the "disorder" or "unpredictability" of a distribution using its entropy. If we wanted, we could compare two distributions just using their entropy — but that wouldn't tell us how similar they were to one another. They could be <em>entirely different,</em> with entirely different events which had entirely different probabilities, and still have the same entropy. Our comparison would only give us a measure of how relatively disordered or unpredictable one distribution is to another.</p>
<p>We want a way to look at two distributions and see how different they are. For example, maybe you have reliable information about the true probability distribution for some set of events, and you want to measure how different your internal predictions were from that true distribution — say, to compare your accuracy to that of your friend's models. (I mean, you usually can't access your internal probabilities for things, so let's pretend you both actually made statistical models of the thing.)</p>
<p>One way to do this is Kullback–Leibler divergence, but people seem to have a hard time pronouncing this (?) so instead they just call it "KL Divergence." This comes up all the time in other fields, for example machine learning — it turns out that it's <em>really useful</em> to be able to compare two distributions!</p>
<p>The intuition for how you calculate KL divergence builds on the concept of surprise and information we built up earlier.</p>
<p>Let's say you want to play a coin toss game — if it's heads you win, if it's tails you lose. Your friend Anansi conveniently has a coin on hand, and offers it to you for the game. Unbeknownst to you, Anansi's coin is unfairly weighted — it's tails 70% of the time and heads only 30% of the time.</p>
<p>Hence, you have the following distributions:</p>
<p>$P$ is the true probability distribution of outcomes given by the weighting of Anansi's coin.</p>
<table>
<thead>
<tr>
<th>Outcome</th>
<th>Heads</th>
<th><strong>Tails</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Probability</strong></td>
<td>0.3</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<p>$Q$ is your model of the distribution of outcomes for that coin toss. (You assume it's a fair coin by default.)</p>
<table>
<thead>
<tr>
<th>Outcome</th>
<th>Heads</th>
<th><strong>Tails</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Probability</strong></td>
<td>0.5</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<p>How do we measure the difference between these two simple distributions? </p>
<p>One way to do this would be to measure the expected <em>additional</em> surprise that we'd receive from playing this game <em>thinking that the outcomes were governed by distribution $Q$ when actually they were governed by $P$.</em> To put it another way, we'd be measuring the additional surprise we get if we think $Q$ <em>instead</em> of $P$ is true.</p>
<p>If you're using model $Q$ when the true probability distribution is $P$, the expected <em>additional</em> surprise for that single event will intuitively be</p>
<ul>
<li>the <em>true</em> frequency of the event ($P(x)$)</li>
<li>times the <em>additional</em> amount you're surprised when it happens using $Q$, relative to the amount you'd be surprised if you used $P$ instead.</li>
</ul>
<p>Formally, you write this as $$P(x)\cdot(\ln(\frac{1}{Q(x)})-\ln(\frac{1}{P(x)}))$$
and if you want to look at this across the whole distribution (each event $x$ in the set of events $X$), you just use the summation</p>
<p>$$\sum_{x \in X}P(x)\cdot(\ln(\frac{1}{Q(x)})-\ln(\frac{1}{P(x)}))$$
giving you the formula for KL Divergence! </p>
<p>(Also, if you're confused by the use of $\ln$ instead of $\log_2$, see footnote <sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. TL;DR the different logarithms don't matter much, you just kinda use whatever's convenient, so I'm switching to $\ln$ here because it's conventional for calculating KL divergence and is hence convenient.)</p>
<p>You might notice that this looks a <em>lot</em> like the entropy formula, which if you recall (now with $\ln$ instead of $log_2$) is $$H(X) =\sum_{x \in X} P(x) \space \ln \frac{1}{P(x)}$$. This similarity should make sense! Remember that entropy measures expected surprise; KL divergence measures expected <em>additional</em> surprise. In fact, another name for KL divergence or expected additional surprise is <em>relative entropy!</em> The only difference is that you're now measuring the divergence of one distribution from another, instead of just the expectations you have about a single distribution in isolation.</p>
<p>Great. Now we can use this to calculate the divergence of our predictions from the true probabilities of heads/tails from Anansi's coin — but first, let's adjust some things in this formula real quick, so that this definition looks like the more standard one on Wikipedia. First, we need to give it a formal function name. Standard is $D_{KL}(P \space ||\space Q)$ ("the KL divergence of P from Q"):
$$D_{KL}(P \space ||\space Q) = \sum_{x \in X}P(x)\cdot(\ln(\frac{1}{Q(x)})-\ln(\frac{1}{P(x)}))$$</p>
<p>Then we do a bit of logarithm algebra, turning the log subtraction into division inside one logarithm: </p>
<p>$$D_{KL}(P \space ||\space Q) = \sum_{x \in X}P(x)\cdot\ln(\frac{\frac{1}{Q(x)}}{\frac{1}{P(x)}})$$
And then we just simplify the fraction using the reciprocals to get our final equation:
$$D_{KL}(P \space ||\space Q) = \sum_{x \in X}P(x)\cdot\ln(\frac{P(x)}{Q(x)})$$
To reaffirm this intuitive derivation, let's get back to playing games with Anansi. Our expected additional surprisal for a single game would be $$\textbf{(1)} \: \: P(\text{Heads})\cdot [\ln\frac{1}{Q(\text{Heads})}-\ln\frac{1}{P(\text{Heads})}]$$$$+ \space \space \space P(\text{Tails})\cdot[\ln\frac{1}{Q(\text{Tails})}-\ln\frac{1}{P(\text{Tails})}]$$</p>
<p>Which we can simplify as follows:</p>
<p>$$\textbf{(2)} \: \: P(\text{Heads}) \cdot \ln(\frac{1/Q(\text{Heads})}{1/P(\text{Heads})}) + P(\text{Tails}) \cdot \ln(\frac{1/Q(\text{Tails})}{1/P(\text{Tails})}) $$ 
$$\textbf{(3)} \: \:P(\text{Heads}) \cdot \ln\frac{Q(\text{Heads})}{P(\text{Heads})} + P(\text{Tails}) \cdot \ln\frac{Q(\text{Tails})}{P(\text{Tails})} $$</p>
<p>Now, we can write it all in decimal form, substituting according to our two-way table </p>
<table>
<thead>
<tr>
<th></th>
<th>Heads</th>
<th>Tails</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>P(x)</strong></td>
<td>0.3</td>
<td>0.7</td>
</tr>
<tr>
<td><strong>Q(x)</strong></td>
<td>0.5</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<p>$$0.3 \cdot \ln(0.5/0.3)+0.7 \cdot\ln(0.5/0.7) \approx 0.08228 $$
To confirm this, we can write a little python script (credit to <a href="https://www.statology.org/kl-divergence-python/">Zach Bobbitt on Statology</a>): </p>
<div class="bw"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">rel_entr</span>

    <span class="n">P</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rel_entr</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">)))</span>
</pre></div>

<p>Which nicely yields <code>0.08228287850505178</code>. :D</p>
<p>There's how you describe and calculate K-L divergence!</p>
<h2>Bounds on algorithmic compression</h2>
<p>Until now, we've mostly talked about information as it relates to surprise, or error, with events out in "the world" — you get information from "the world" when your predictions about it don't match up to reality. However, "the world" doesn't have to be, say, reality as a whole (in the style of a bayesian agent that does predictive processing/active inference, which I'll talk about in another post or something): we can zoom into specific kinds of events that are modeled by probability, and treat those as if they are our entire "world" — we just need to slightly adjust the way we apply our intuitions. </p>
<p>In particular, here I'm thinking about applying information theory to data transfer and compression. It's useful and interesting — though it is somewhat limited in its results, for reasons related to the assumptions of Shannon's coding theorem (in particular, that the sequence we're talking about was generated probabilistically, with each character coming from the same random variable defined by the same distribution) which I'll talk about more below. But I think this is a great starting point for lots of other interesting ideas I'd like to get to, like Kolmogorov complexity, and, surprisingly, diagonalization arguments like Gödel's incompleteness theorems.</p>
<h3>Shannon's source coding theorem: probabilistic/unstructured information</h3>
<p>Let's say we have a probabilistic event, and we want to store a history of its occurrences. We can return to our scenario with Anansi for this.</p>
<p>Anansi finally told you that the coin wasn't fair, so you decided to play a game with a four-sided die that he had in his pocket instead. (A pyramidal die, basically — look it up if you've never seen a d4 before.) You'll roll the die a bunch of times and then tally up your score after 20 rolls — so you need to save the results of your rolls in  a string. For convenience, the die has numbers 0 to 3 on it instead of 1 to 4.</p>
<p>You could store it as follows: </p>
<div class="bw"><pre><span></span><span class="mi">3</span> <span class="mi">2</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">1</span> <span class="mi">3</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">3</span> <span class="mi">0</span> <span class="mi">2</span>
</pre></div>

<p>In a computer that would be stored in bits (spaces added for readability; in reality there are no spaces between numbers): </p>
<div class="bw"><pre><span></span><span class="mi">11</span> <span class="mi">10</span> <span class="mi">01</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">01</span> <span class="mi">11</span> <span class="mi">01</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">11</span> <span class="mi">00</span> <span class="mi">10</span>
</pre></div>

<p>But let's say, for the sake of illustration, our storage space is super limited. Is there way to <em>compress</em> this string — any shorter representation of it that could still save it exactly?</p>
<p>The answer is, basically, "not really." <strong>Shannon's source coding theorem</strong> formalizes this. <em>Source coding</em> is what we call it when we encode the output of an information source into symbols from an alphabet, usually bits, such that it's <em>invertible,</em> i.e. you can undo the encoding to get the original output of the information source. For our current purposes, this information source needs to be discrete, like dice rolls or words in spoken English, not a continuous function. (You <em>can</em> extend these ideas to continuous functions, but that's a separate domain called rate-distortion theory, which I currently know nothing about.)</p>
<p>Shannon's source coding theorem states that (a) the upper bound on efficiency of compression for a discrete information source generated by a random variable is the <em>entropy</em> of that random variable, and (b) you can create encodings that get arbitrarily close to that bound.</p>
<p>Let's unpack that a bit. (A) is placing a bound on how well you can compress a sequence like the one we have above. The actual mathematical formulation says that the <em>average number of bits per symbol</em> can't be less than the <em>entropy of the variable.</em></p>
<p>Let's walk through an example using the 4-sided die and our formulas from above. The entropy of our 4-sided die — <em>assuming it's fair</em> — is the sum over each possible outcome's probability times the information it would give us. (Note that I'm going back to $\log_2$ because we're using bit representations of the code and it's nice if we use that.)</p>
<p>$$ \textbf{(1)} \space \space  H(\text{4-sided die}) = \sum_{x \space \in \space \text{outcomes}} P(x) \cdot \log_2 \frac{1}{P(x)}$$
$$ \textbf{(2)} \space \space H(\text{4-sided die}) = 4 (0.25 \log_2(1/0.25) $$
$$  \textbf{(3)} \space \space H(\text{4-sided die}) = \log_2(4)= 2 \: \text{bits}$$
So our entropy is 2 bits. As you saw above, we were storing our scores in two-bit numbers (spaces added for readability): </p>
<div class="bw"><pre><span></span><span class="mi">11</span> <span class="mi">10</span> <span class="mi">01</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">01</span> <span class="mi">11</span> <span class="mi">01</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">11</span> <span class="mi">00</span> <span class="mi">10</span>
</pre></div>

<p>Which is great! That means we have an ideal encoding — the <strong>code rate</strong> (this is the technical term for "number of bits in the encoding, per symbol in the original source.") is 2 bits, which is equal to the entropy.</p>
<p>So what (a) in Shannon's source coding theorem is saying is that no matter how good our encoding is, we'll never be able to find an encoding that has a lower code rate than the entropy of the source — that is, without losing information. (If we're willing to compress lossily, we might be able to get below that bound.)</p>
<p>What (b) is saying is that, no matter the discrete source, we can always find an encoding that gets the code rate arbitrarily close to the source's entropy. For the situation with a fair 4-sided die, we by default have a coding that's ideal; that's usually not the case.</p>
<p>Back to you and Anansi. You clearly should have expected that Anansi would have an unfair die, if he already had an unfair coin on hand. After playing with him for a while, you begin to really suspect that the die isn't fair. You press him on it, and he finally reveals the true distribution of values:</p>
<table>
<thead>
<tr>
<th>Value (x)</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>P(x)</strong></td>
<td>0.125</td>
<td>0.125</td>
<td>0.5</td>
<td>0.25</td>
</tr>
</tbody>
</table>
<p>So that means the entropy isn't actually 2. Let's calculate it again: </p>
<p>$$ H(\text{Unfair die}) = 2(0.125\log_2 \frac{1}{.125}) + 0.5\log_2\frac{1}{.5} + 0.25 \log_2 \frac{1}{0.25}$$
$$ H(\text{Unfair die}) = 2(1/8\cdot\log_2 8) + 1/2 \cdot\log_22 + 1/4 \cdot \log_2 4$$
$$ H(\text{Unfair die}) = 2(3/8) + 1/2 + 1/2 = 1.75 \: \text{bits}$$</p>
<p>It's lower, since it's a little less random. How do we create a coding that gets closer to this lower bound?</p>
<p>There are a couple options for simple data compression algorithms. I sort-of ranged them from, like, least advanced to most advanced:</p>
<ul>
<li><strong>Run-length encoding</strong></li>
<li><strong>Shannon-Fano encoding</strong></li>
<li><strong>Huffman encoding</strong></li>
<li><strong>Arithmetic coding</strong></li>
<li><strong>Lempel-Ziv coding</strong></li>
<li>...others, too many to name</li>
</ul>
<p>But, uh, I don't really want to spend a bunch of hours figuring out and then explaining these encoding methods right now. (I only really know at this point how to write out the first three; I just thought it might be nice for later to have a little map so I can learn more about data compression when I feel like it.) For now I'll just give an example of how we can get a code rate lower than two — I'm not going to show how to get arbitrarily close to the entropy because I don't know how to, and am not currently prioritizing it.</p>
<p>Anyway, for a couple reasons (including that it is simple and fast both to comprehend and to implement) we can use Huffman coding to achieve a better compression ratio. (From brief searching, it looks like Arithmetic coding is better at getting close to optimal on compression, but I am not locked in on compression algorithms right now.)</p>
<p>Huffman coding is pretty intuitive for a distribution like ours, where all the options are distributed nicely as powers of 2. What we can do is give new representations to the numbers; more frequent outcomes get shorter encodings. We'll assign them like follows: </p>
<table>
<thead>
<tr>
<th><strong>Value</strong></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Probability</strong></td>
<td>1/8</td>
<td>1/8</td>
<td>1/2</td>
<td>1/4</td>
</tr>
<tr>
<td><strong>Encoding</strong></td>
<td>000</td>
<td>001</td>
<td>1</td>
<td>01</td>
</tr>
</tbody>
</table>
<p>Since 2 is the most likely, it gets the shortest encoding — <code>1</code>. Note that I added spaces to the binary representations of the sequence above for readability; in reality our old sequence looked like <code>111001101110100111011010101010101111001</code>, with no spaces. Since we don't have dividers between characters, and our Huffman code is variable-length (the encodings range from 1 bit per symbol to 3 bits) we need to have a <strong>prefix-free code</strong>: no symbol's coding is the "prefix" of another symbol's coding, i.e. we couldn't have a character encoded as <code>00</code> and another as <code>001</code>; we wouldn't be able to tell where one character started and another ended.</p>
<p>Hence, since this needs to be a prefix-free code, we can't use <code>0</code> for 3, even though it's the second-most likely outcome. Instead, we use <code>01</code>. Similarly, we can't use <code>11</code> or <code>010</code> for 0 and 1; instead we use <code>001</code> and <code>000</code>. (with larger symbol sets, the process looks like <code>1</code>, <code>01</code>, <code>001</code>, <code>0001</code>, etc. until you finally get a string of all 0s and a 1, and then a string of all 0s.)</p>
<p>Anyway, so we have our sequence:</p>
<div class="bw"><pre><span></span><span class="mi">3</span> <span class="mi">2</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">1</span> <span class="mi">3</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">3</span> <span class="mi">0</span> <span class="mi">2</span>
</pre></div>

<p>Its old standard 2-bit encoding, with code rate 2 (again, spaces added for readability): </p>
<div class="bw"><pre><span></span><span class="mi">11</span> <span class="mi">10</span> <span class="mi">01</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">01</span> <span class="mi">11</span> <span class="mi">01</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">11</span> <span class="mi">00</span> <span class="mi">10</span>
</pre></div>

<p>And its Huffman coding with code rate 33/20 = 1.65: </p>
<div class="bw"><pre><span></span><span class="mi">01</span> <span class="mi">1</span> <span class="mi">001</span> <span class="mi">1</span> <span class="mi">01</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">001</span> <span class="mi">01</span> <span class="mi">001</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">01</span> <span class="mi">01</span> <span class="mi">000</span> <span class="mi">1</span>
</pre></div>

<p>Wait, why is the code rate lower than the entropy‽</p>
<p>We got lucky! Shannon's source-coding theorem tells you that the <em>expected</em> code rate can't be lower than the entropy — but for individual strings it can. As an intuition here, in theory Anansi's die could roll 20 twos in a row (it would just be very unlikely), and thus we'd have a code rate of 1!</p>
<p>What we need to do is instead calculate the expected, or <em>average</em> code length for our encoding method in general, rather than computing the code length for a specific string.</p>
<p>The formula for expected code length: </p>
<p>$$ \sum_{x \in X} P(x) \cdot \text{len}(x)$$
Hence, for our code: </p>
<p>$= P(0) \cdot \text{len}(000) + P(1) \cdot \text{len}(001) + P(2) \cdot \text{len}(1) + P(3) \cdot \text{len}(01)$
$= 0.125 \cdot 3 + 0.125 \cdot 3 + 0.5 \cdot 1 + 0.25 \cdot2$
$= 0.375 + 0.375 + 0.5 + 0.5$
$= 1.75 \: \text{bits}$</p>
<p>There we go! There's our entropy. :)</p>
<p>Now, the formalization and proof of this theorem requires a lot more concepts than we currently have — we need mutual information, conditional entropy, and ideas about communication channels and channel capacity. </p>
<h4>Formalizing the source coding theorem</h4>
<p><em>TODO: add formalization using mutual information, channel capacity, conditional entropy</em> (eventually this will cover everything in the wikipedia hopefully), Kraft inequality, gibbs inequality, etc.</p>
<h4>Why this coding theorem isn't enough</h4>
<p>There's a glaring weakness to this theorem's applicability, though: it can only talk about strings generated from probabilistic distributions! One of the key insights that I've taken away from learning about applications of information theory to computer science has been that <strong>randomness is incompressibility.</strong> We saw this above: a uniform distribution was <em>more random</em> than the unfair die that Anansi gave us; it had higher entropy. I think this insight points us further towards the limitations of Shannon's theorem.</p>
<p><em>Structured information — data structures, programs, coherent strings, etc. — is more compressible than stochastically-generated information.</em> Being deliberately structured means that it's less random. If a human wrote a program, and you wanted to compress it, the Shannon entropy of the stringified program would tell us very little about the limits of compressing it; Shannon's theorem can only take into account a minimal amount of structure — that is, repetition of substrings.</p>
<p>If we want to talk about the compressability, entropy, randomness, of logically-structured information, we need a new vocabulary.</p>
<h3>Algorithmic information theory and Kolmogorov complexity: Non-random, structured information</h3>
<p>To talk about information involved in computation, instead of information generated stochastically — data structures instead of random outputs — we start to get into algorithmic information theory (AIT). Wikipedia calls it a branch of theoretical computer science. To take their quote: </p>
<blockquote>
<p>According to Gregory Chaitin, it is "the result of putting Shannon's information theory and Turing's computability theory into a cocktail shaker and shaking vigorously."</p>
</blockquote>
<p>AIT mostly studies strings — how to measure their complexity, and what to do with those measures. This is nice because lots of math can be described in terms of strings; it gives us extra perspective on this math, I guess. (As of writing this, I don't really have a good intuition for what this actually means.)</p>
<p>I'm drawing here from three main sources: </p>
<ul>
<li>Wikipedia (<a href="https://en.wikipedia.org/wiki/Algorithmic_information_theory">Algorithmic Information Theory</a> and <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov Complexity</a>)</li>
<li><em>Kolmogorov Complexity and Algorithmic Randomness</em> by Shen, Uspensky, and Vereshchagin (ISBN 978-1-4704-7064-7)</li>
<li>And of course, LLMs for helping me orient myself and clarify the above.</li>
<li><a href="https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction">An Intuitive Explanation of Solomonoff Induction</a></li>
</ul>
<p>The first notion to define is <strong>Kolmogorov Complexity</strong>, abbreviated below as <strong>K-complexity</strong>, and (from Wikipedia) "also known as algorithmic complexity, Solomonoff–Kolmogorov–Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy" along with the other 93 names of god. Heuristically or something, we should think that this is an important concept because it has so many names. Going into it, though, keep in mind one of its names: <em>algorithmic entropy</em>. The fact that this is another measure of entropy, a different way of looking at a similar fundamental idea, should maybe sit in the back of your mind as we proceed.</p>
<p>I think the intuition of K-complexity is pretty simple. <strong>On a high level of abstraction, the K-complexity of a string is the length of the shortest program you could write to generate that string.</strong> You can also think of this as the K-complexity as being defined by "the shortest possible <em>description</em> of a string" in some formal sense; it's the same intuition. </p>
<p>To formalize this, we'll introduce a couple ideas.</p>
<p>First, a decompressor: literally just a program that takes in a string (usually $x$) and outputs another string (usually $y$). It's a decompressor because it's a program that takes a description of a string $x$ — i.e. a compressed version of a string — and turns it into the string itself, $y$. We write this as $D(x) = y$. A</p>
<p>The decompressor can also be called a "description mode" or "description language," which in many cases is more intuitive. For example (though informally), "the numbers 0 and 1 repeated ten times" is a description ($x$) of the string <code>01010101010101010101</code> ($y$). In this case, our decompressor is something like "turn the natural language description into the string." </p>
<p>To make this a little more formal, we can use a different description mode: Python, instead of natural language. The string <code>print("01" * 10)</code> when run by a python interpreter outputs our string, <code>01010101010101010101</code>. Hence, Python is our 'description mode'; when we evaluate $x$ (our program) with $D$ (our description mode) we get $y$ (our desired string).</p>
<p>Hence why it's a "description mode" — the decompressor is <em>a way of turning the description into the thing being described</em>. Every description has meaning only in the context of a description mode — without a description mode, there are no rules given to allow the description to "make sense". There need to be formally-defined rules to evaluate the description (otherwise you could just make up a random string and say it was a perfect description for any other string). So, to be clear, while I used the natural language example above, you couldn't just use that as a description mode straightforwardly; <strong>a description mode must be a computable function.</strong></p>
<p>The K-complexity of a string $y$ with respect to a specific description mode $D$ is the length of the string's shortest description ($x$). Formally:</p>
<p>$$ C_D(y) = \min \:{l(x) \: | \: D(x) = y}$$
Where $l(x)$ denotes the length of $x$. (notation used by the authors; I used $\text{len}(x)$ above but I don't want to have to translate notation all the time in the future. Sorry for changing notation all the time lol. Let's just say it's different perspectives on the same idea!) </p>
<p>(There is a lot of random notation used for K-complexity, so to be honest this is fitting. The book uses the above $C_D$ and it feels nice and intuitive to me — complexity with respect to the description mode D — so I'll go with that. You might also see for example $K(x)$ (for Kolmogorov) or $H(x)$, reusing the notation for entropy since this is algorithmic entropy — both of these were used by Kolmogorov himself. To quote the authors: "Unfortunately, the notation and terminology related to Kolmogorov complexity is not very logical (and different people often use different notation). Even the same authors use different notation in different papers" (<em>xii</em>).) </p>
<p>Typically we'll place formal limitations on what our $D$ can be. To make things nice we'll say that $D$ is a function that takes a binary string and outputs a binary string (since we can encode all strings as a binary string, usually trivially). One way to notate this is to say $\Xi = {0,1}^{*}$ (i.e. $\Xi$, Xi, represents the set of all binary strings), and to, say, define $D$ as $D: \Xi → \Xi$ There are more details given by computability theory which we could go into an infinite rabbit hole describing, but I'll leave that there for now.</p>
<p>One question that this definition left me with was, <em>why can't you just make up a description mode and hide all the complexity in that?</em> Then you could have a program with length 0 that deterministically produces your desired output, and this notion becomes kind of useless. We can't answer this now, but we'll be able to in a moment.</p>
<h2>A universal decompressor</h2>
<p>We say that a decompressor/description mode $D_1$ is not worse than another $D_2$ if its output for all strings $x$ differs only by a constant, $c$. Written formally, $D_1$ is not worse than $D_2$ if 
$$C_{D_1}(x) \leq C_{D_2}(x)+ c.$$
<strong>Theorem</strong>. There is a description mode $D$ that is not worse than any other one: for every description mode $D'$ there is a constant c such that $$C_D(x) \leq C_{D'}(x)+ c$$ for every string x. This constant $c$ might be different for every $D'$, but it will still be a constant. (Spoiler: this is a universal turing machine.)</p>
<p style="text-align: center">. . .</p>
<p><strong>Brief interlude: Big-O notation</strong></p>
<p>(sources: <a href="https://stackoverflow.com/questions/1909307/what-does-on-mean">stack overflow</a>, <a href="https://robbell.io/2009/06/a-beginners-guide-to-big-o-notation">Rob Bell</a>)</p>
<p>We use Big-O notation to relate the length of computation to the size of the input.</p>
<ul>
<li>$\mathcal{O}(1)$ means it takes a constant amount of time to compute an output. </li>
<li>$\mathcal{O}(n)$ means it takes an amount of time proportional to the number of items/size in the input</li>
<li>$\mathcal{O}(n^2)$ means it takes an amount of time proportional to the <em>square</em> of the items — e.g. an algorithm that computes something for each item in a list relative to each other item (like self-attention).</li>
<li>$\mathcal{O}(n \log n)$ <a href="https://robbell.io/2009/06/a-beginners-guide-to-big-o-notation">might be e.g. binary search</a>.</li>
</ul>
<p>This is describing the <em>worst case</em> — it's an upper bound on the function.(Hopefully this is correct. the stack overflow answer + <a href="https://en.wikipedia.org/wiki/Big_O_notation">wikipedia page</a> indicate that there are lots of other notations but I don't really care about them right now.)</p>
<p style="text-align: center">. . .</p>
<p>I think the idea behind "bounded by a constant" is related ish to the vibes of Big-O notation. The constant can be very large, but it will be a constant. If your algorithm is highly complex (like $\mathcal{O}(2^{n^n})$ or something, I don't know, I'm making this up) this constant can be in a sense "negligible." Quoting the authors: </p>
<blockquote>
<p>One could say that such a tolerance makes the complexity notion practically useless, as the constant $c$ can be very large. However, nobody managed to get any reasonable theory that overcomes this difficulty and defines complexity with better precision.</p>
</blockquote>
<p>Anyway, the way we set up a proof for this theorem is just by prefixing any string we input with a binary form of the algorithm we use to decompress it normally. Literally we just use
$$D(Py)=P(y)$$ where $Py$ is just the program $P$ (a description mode used to evaluate $y$). Since description modes are computable functions, we must be able to encode them on the tape. An important note here though is that $P$ must be <strong>self-delimiting</strong>, i.e. we must be able to tell unambiguously where $P$ ends and $y$ begins. This is not obviously the case for all algorithms; programs written in Python, for example, are not self-delimiting; there's no end marker in the programs. You can add comments of arbitrary length that don't affect the program's execution at the end. You can think of it this way: if you concatenate two Python files, it's not always going to be clear where one ends and the next begins.</p>
<p>We can make any $P$ self-delimiting, though, relatively trivially: you take its binary form, double every bit (e.g. <code>010011</code> becomes <code>001100001111</code>) and then append the digits <code>01</code> at the end. This way, you can simply evaluate the doubled digits in bit-pairs; each bit-pair will either be <code>00</code> or <code>11</code> and then when you encounter the bit-pair <code>01</code> you know that $P$ has ended and $y$ is beginning. </p>
<p>Anyway, if we set up this description mode $D$ that just evaluates $Py$ for any $P$ and any $y$, we can say that if $y$ is the shortest description of $x$ (a string) with respect to $P$, $Py$ is also a description of $x$ with respect to $D$. </p>
<p>The shortest description of $x$ with respect to $D$ is at <em>most</em> $Py$ (there may or may not exist some shorter description), and hence the shortest description of $x$ with respect to $D$ is <em>at most</em> longer, by the length of $P$, than the shortest description of $x$ with respect to $P$. Formally: $$ C_D(x) \leq C_P(x) + l(P).$$where $l(P)$ is the length of our encoding of $P$ in binary (which may be altered from some original $P$ such that it is self-delimiting or whatever, in which case this might look something like $2\cdot l(P_{\text{original}}) + 1$, but in which case it would be linear so it doesn't really matter). </p>
<p>Hence, $D$ is a "universal decompressor" or "<strong>universal algorithm</strong>": it is "not worse" than any other decompressor. We say that this description mode $D$ is <em>optimal</em>. </p>
<p>Now we can answer the question from earlier: <em>why can't you just make up a description mode and hide all the complexity in that?</em> Well, <em>you can</em>. But it doesn't make K-complexity trivial, because when you're comparing this description you just made up to others using this optimal algorithm, <em>the description mode is encoded in the string too!</em> </p>
<h2>Turing machines and Solomonoff induction</h2>
<p>This whole time we've been secretly working with <strong>Turing machines</strong>. Turing machines provide a neat way to formalize the notion of "algorithm," which I'll talk about now. </p>
<p>In principle, all information can be represented as a single binary sequence. It might be very long, very complex, very confusing, but <em>in principle</em>, anything that we can describe mathematically — from neuron cells to sound waves to physical particles — can be represented as a binary sequence. If we assume that the universe is actually a deterministic process (I mean, maybe a big assumption) then the entire universe could be represented as a binary string.</p>
<p>Turing machines are extremely simple, formal computers that operate on binary strings. You can think of it as a little machine sitting on a tape of infinite length; it can read a single binary character at a time, write a single character at a time, and move left or right one character at a time. </p>
<h3>Formalizing Turing machines</h3>
<p>We can formalize it with this absolutely unhinged notation: $$ M = \langle Q, \Gamma, b, \Sigma, \delta, q_0, F \rangle.$$(which I got from Wikipedia, obviously. Note that there are many different ways to formalize the Turing machine, but they all are isomorphic.)</p>
<ul>
<li>$\Gamma$ (Gamma) is the finite alphabet of symbols you can put on the tape, among them $b$, the blank symbol — this is what's on the tape before it's written as 0 or 1, and since the tape is infinite, this symbol occurs infinitely — and $\Sigma$, the set of input symbols (here 0 and 1).</li>
<li>$Q$ is the finite (and non-empty) set of states that the machine can be in, among them $q_0$, the initial state, and $F$, the set of of acceptable final states.</li>
<li>$\delta$ is the "transition function" of the Turing machine: $$\delta: (Q \setminus F) \times \Gamma  \rightharpoonup Q\times \Gamma \times {L,R}$$It takes inputs of $(Q \setminus F) \times \Gamma$ (i.e. it takes in tuples, one item being from the set $Q$ excluding the set of final states $F$, and the other being from $\Gamma$) — in other words, it knows its current state and the character that it's reading from the tape — and its output is $Q \times \Gamma \times {L, R}$, i.e. it specifies in its output (1) the next state the machine will be in, (2) what character to overwrite onto the current character it's reading, and (3) the next movement, either one to the right or one to the left. This function seems to be usually defined by a "state table" for simpler machines. I don't know what it looks like for more complex machines — or if that's the only way to define it.</li>
</ul>
<p><em>Little note since I wondered what this meant:</em> Wikipedia describes this as a "partial function". A partial function from set $X$ to set $Y$ just maps a <em>subset</em> of $X$ onto the whole of $Y$. $\delta$ is a partial function because it only uses a subset of $Q$ as an input. (full definition reiterated just because I still feel fancy using kinda obscure latex symbols)</p>
<p>Wikipedia has lots of other good stuff about Turing machines. For example: </p>
<blockquote>
<p>In the words of van Emde Boas (1990), p. 6: "The set-theoretical object [his formal seven-tuple description similar to the above] provides only partial information on how the machine will behave and what its computations will look like."</p>
<p>For instance,</p>
<ul>
<li>There will need to be many decisions on what the symbols actually look like, and a failproof way of reading and writing symbols indefinitely.</li>
<li>The shift left and shift right operations may shift the tape head across the tape, but when actually building a Turing machine it is more practical to make the tape slide back and forth under the head instead.</li>
<li>The tape can be finite, and automatically extended with blanks as needed (which is closest to the mathematical definition), but it is more common to think of it as stretching infinitely at one or both ends and being pre-filled with blanks except on the explicitly given finite fragment the tape head is on (this is, of course, not implementable in practice). The tape <em>cannot</em> be fixed in length, since that would not correspond to the given definition and would seriously limit the range of computations the machine can perform to those of a <a href="https://en.wikipedia.org/wiki/Linear_bounded_automaton" title="Linear bounded automaton">linear bounded automaton</a> if the tape was proportional to the input size, or <a href="https://en.wikipedia.org/wiki/Finite-state_machine" title="Finite-state machine">finite-state machine</a> if it was strictly fixed-length.</li>
</ul>
</blockquote>
<p>They also have a <a href="https://en.wikipedia.org/wiki/Turing_machine_examples">list of examples of Turing machines</a>. The one they provide in the article is clear and good though:</p>
<p><img alt="busybeaverturing.png" src="busybeaverturing.png" /></p>
<h3>Turing machines formalize the notion of "algorithm"</h3>
<p>Every other way we've tried to formalize the idea of an "algorithm" has been either equivalent to or weaker than a Turing machine; weaker formalizations include finite-state machines and pushdown automata — I don't currently understand why these are weaker, but it's not a priority for me to understand this statement in particular at the moment so I'll put it on the backburner — and some equivalent formalizations include Alonzo Church's Lambda Calculus, some Cellular automata (such as <a href="https://en.wikipedia.org/wiki/Rule_110">Rule 110</a> and Conway's <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Game of Life</a>, both of which can simulate a Turing machine). Something that is <strong>Turing-complete</strong> can simulate a Turing machine. </p>
<p>The three most promising approaches to formal computability — lambda calculus ($\lambda$-calculus), Turing machines, and general recursion (not sure what that is but everyone says it's important) — turned out to all be equivalent, and since subsequent attempts have been either equivalent or worse, this gave rise to the <strong>Church-Turing Thesis:</strong> that a function on the natural numbers is "effectively calculable" iff it is computable by a Turing machine. </p>
<p>Effectively calculable is just a way of saying "something you can calculate, with some intuitive sense of the word 'calculate'". Here's how Turing explains the term: </p>
<blockquote>
<p>We shall use the expression "computable function" to mean a function calculable by a machine, and let "effectively calculable" refer to the intuitive idea without particular identification with any one of these definitions.
(<a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis#Statement_in_Church's_and_Turing's_words">Wikipedia</a>, apparently from Turing's PhD thesis)</p>
</blockquote>
<p>It's something like, "effectively calculable means that a human could do it with pen and paper and a simple set of algorithms given infinite time, attention, food, etc." So basically, the idea is that any computation that can be... actually <em>done</em> in some sense can be done by a Turing machine (though not necessarily one that we actually build, because the machine might be too big to fit in the universe or something). Note again that the Church-Turing thesis is a conjecture: it has not been proven. I'm not sure what a proof would even look like, though to be fair that's probably the case for many provable things that haven't been proven yet.)</p>
<p>To reiterate one last time: </p>
<blockquote>
<p>Remember how limited the states of a Turing machine are; every machine has only a finite number of states with “if” rules like in the figure above. But somehow, using these and the tape as memory, they can simulate every set of rules, every algorithm ever thought up. Even the distinctively different theory of quantum computers is at most Turing complete. In the 80 years since Turing’s paper, no superior systems have been found. The idea that Turing machines truly capture the idea of “algorithm” is called the Church-Turing thesis.
(<a href="https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction">LessWrong</a>)</p>
</blockquote>
<p>There are some cool things on the <a href="https://en.wikipedia.org/wiki/Hypercomputation">Hypercomputation</a> Wikipedia page, which I'd recommend checking out. Eventually I'd want to go down that rabbit hole, but for now I will leave that and return to Solomonoff and Kolmogorov.</p>
<p>Anyway, Turing proved that there's one specific Turing machine that can simulate all other Turing machines: the "universal Turing machine." To make it simulate all the other machines, all you need to do is give it the <em>compiler</em> — funny that I didn't actually know this word before, but a compiler is a program that translates code between a source language and a target language, usually between a high-level language and a low-level language, but can be between any language — that translates between whatever machine the universal Turing machine is simulating, and the universal Turing machine itself. You just prepend it to the input — you give the universal Turing machine the compiler, then the input. </p>
<p>Sound familiar? I think this is isomorphic, conceptually, to our optimal description mode we proved the existence of earlier. </p>
<p>The Turing machine takes binary sequences as input, and leaves behind a different binary sequence as output.</p>
<p>If the notion of algorithms is formalized by Turing machines, and all Turing machines can be simulated by the universal Turing machine, then every algorithm can be simulated by the universal Turing machine. Any conversion from any string to any other string can be simulated by the universal Turing machine — in fact, that's <em>just what it's doing.</em></p>
<h2>Solomonoff induction</h2>
<p>What can we do with this fact? We kind systematically find truth about the world. You treat algorithms as hypotheses about the processes that govern the world. </p>
<p>One way a time-step physics simulation might work might be that it takes in (a) inputs of the states of the objects it's simulating and (b) the rules or algorithm that it should use to predict the next state, and then outputs a predicted "next state" of the world. </p>
<p>I'm feeling stuck here. </p>
<ul>
<li>
<p>solomonoff induction gives a universal prior for stuff based on this relationship with the complexity of the shortest hypothesis that generates it</p>
</li>
<li>
<p>formal turing machines</p>
</li>
<li>how do turing machines actually work</li>
<li>use the solmonoff induction post, then church-turing thesis, and then take the wikipedia definition and maybe some other things</li>
<li>the halting problem</li>
<li>lambda calculus</li>
<li>computability theory</li>
<li>how are weaker formalizations of "algorithm" weaker</li>
<li>https://en.wikipedia.org/wiki/Hypercomputation — wtf? <a href="https://en.wikipedia.org/wiki/Chaitin%27s_constant">chiatin's constant</a><ul>
<li>just stuff that https://en.wikipedia.org/wiki/Gregory_Chaitin has wokred on</li>
<li>He is today interested in questions of <a href="https://en.wikipedia.org/w/index.php?title=Metabiology&amp;action=edit&amp;redlink=1" title="Metabiology (page does not exist)">metabiology</a> and <a href="https://en.wikipedia.org/wiki/Information_theory" title="Information theory">information-theoretic</a> formalizations of the theory of <a href="https://en.wikipedia.org/wiki/Evolution" title="Evolution">evolution</a>, and is a member of the Institute for Advanced Studies at <a href="https://en.wikipedia.org/wiki/Mohammed_VI_Polytechnic_University" title="Mohammed VI Polytechnic University">Mohammed VI Polytechnic University</a>.</li>
<li>Chaitin claims that <a href="https://en.wikipedia.org/wiki/Algorithmic_information_theory" title="Algorithmic information theory">algorithmic information theory</a> is the key to solving problems in the field of <a href="https://en.wikipedia.org/wiki/Biology" title="Biology">biology</a> (obtaining a formal definition of 'life', its origin and <a href="https://en.wikipedia.org/wiki/Evolution" title="Evolution">evolution</a>) and <a href="https://en.wikipedia.org/wiki/Neuroscience" title="Neuroscience">neuroscience</a> (the problem of <a href="https://en.wikipedia.org/wiki/Consciousness" title="Consciousness">consciousness</a> and the study of the mind).</li>
</ul>
</li>
<li>history of the Entscheidungsproblem and its subseqent </li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>As a fun little note, the letter $H$ is standard notation after it was used by Shannon. Apparently it was originally supposed to be the greek letter Eta — which looks exactly the same as H, such that LaTeX doesn't even have a separate symbol for it, you're just supposed to use <code>$H$</code> — which is <a href="https://math.stackexchange.com/questions/84719/why-is-h-used-for-entropy">apparently what Boltzmann used originally</a> used originally for thermodynamic entropy, since the letter E was already taken for other things.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>I told you earlier we would be working in $\log_2$ because it made sense in context. However, turns out that the units don't matter much for your calculation, and there's no "standard" unit of information. However, from what I've seen, most KL divergence calculators use $\ln$ instead of $\log_2$ or $\log$, since in various other places in statistics the $\ln$ function is more common, and therefore using "nats" of information (the unit when we calculate using log base $e$ as opposed to log base 2) allows for easier simplification of calculations. <strong>Hence, for now, I'm going to switch to using nats and $\ln$ instead of bits and $\log_2$.</strong>&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
            </div>
            <script type="text/javascript" src="mathjax.js"></script>
            <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

        </body>
        </html>
        